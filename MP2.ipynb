{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datos #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Perceptron multicapa ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()  # Convierte la imagen de un formato PIL o numpy.ndarray a un tensor.\n",
    "])\n",
    "\n",
    "mnist_dataset = datasets.MNIST(\n",
    "    root='./data', train=True, transform=transform, download=True\n",
    ")\n",
    "\n",
    "# Creamos un DataLoader que nos permite cargar los datos en lotes pequeños.\n",
    "# `dataset=mnist_dataset` es el dataset que se cargará.\n",
    "# `batch_size=16` indica que cada lote contendrá 16 imágenes y etiquetas.\n",
    "# `shuffle=True` mezcla los datos aleatoriamente en cada época, mejorando la generalización del modelo.\n",
    "data_loader = DataLoader(\n",
    "    mnist_dataset, batch_size=16, shuffle=True\n",
    ")\n",
    "\n",
    "# Obtenemos un único lote de datos del DataLoader.\n",
    "# `next(iter(data_loader))` convierte el DataLoader en un iterador y toma el primer lote.\n",
    "# El lote contiene `images` (los tensores de las imágenes) y `labels` (las etiquetas correspondientes).\n",
    "images, labels = next(iter(data_loader))\n",
    "\n",
    "\n",
    "# Plot the images in a grid\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(16):\n",
    "    plt.subplot(4, 4, i + 1)\n",
    "    plt.imshow(images[i].squeeze(), cmap='gray')\n",
    "    plt.title(f'Label: {labels[i].item()}')\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arquitectura # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# para crear, entrenar y evaluar redes neuronales.\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        # Inicializamos la clase base nn.Module\n",
    "        # Esto habilita funciones esenciales como la gestión de capas y forward pass.\n",
    "        super(MLP, self).__init__()\n",
    "        # Capa completamente conectada: de entrada (28x28 píxeles) a 512 neuronas\n",
    "        self.fc1 = nn.Linear(28 * 28, 512)\n",
    "        # Capa oculta: de 512 neuronas a 256 neuronas\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        # Capa oculta: de 256 neuronas a 128 neuronas\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        #Capa de salida: de 128 neuronas a 10 clases (números del 0 al 9)\n",
    "        self.fc4 = nn.Linear(128, 10)\n",
    "        # Función de activación Sigmoid\n",
    "        self.ReLU =  nn.ReLU()\n",
    "        # Dropout para evitar sobreajuste\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    # Definimos cómo pasa la información a través de la red\n",
    "    # Este método es obligatorio en las clases que heredan de nn.Module.\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)  # Aplanamos las imágenes (de 28x28 a 1D)\n",
    "        x = self.ReLU(self.fc1(x))  # Aplicamos la primera capa y Sigmoid\n",
    "        x = self.dropout(x)         # Aplicamos Dropout\n",
    "        x = self.ReLU(self.fc2(x))  # Aplicamos la segunda capa y Tahn\n",
    "        x = self.dropout(x)         # Aplicamos Dropout\n",
    "        x = self.ReLU(self.fc3(x))\n",
    "        x = self.fc4(x)  # Aplicamos la capa de salida\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiperparámetros\n",
    "batch_size = 64 # Tamaño de lote\n",
    "learning_rate = 0.1 # Tasa de aprendizaje\n",
    "epochs = 10       # Número de épocas de entrenamiento\n",
    "\n",
    "# Preprocesamiento y carga de datos de MNIST\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),                 # Convertimos imágenes a tensores\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalizamos a media 0 y varianza 1\n",
    "])\n",
    "train_dataset = datasets.MNIST(\n",
    "    root='./data', train=True, transform=transform, download=True)  # Dataset de entrenamiento\n",
    "test_dataset = datasets.MNIST(\n",
    "    root='./data', train=False, transform=transform, download=True)  # Dataset de prueba\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset, batch_size=batch_size, shuffle=True)  # Dataloader para entrenamiento\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset, batch_size=batch_size, shuffle=False)  # Dataloader para prueba\n",
    "\n",
    "# Definimos el modelo, la función de pérdida y el optimizador\n",
    "model = MLP()                             # Creamos una instancia del modelo MLP\n",
    "criterion = nn.CrossEntropyLoss()         # Función de pérdida para clasificación\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)  # Optimizador Adam\n",
    "\n",
    "# Bucle de entrenamiento\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # Ponemos el modelo en modo entrenamiento\n",
    "    for images, labels in train_loader:  # Iteramos sobre lotes de datos\n",
    "        optimizer.zero_grad()            # Reiniciamos los gradientes\n",
    "        outputs = model(images)          # Hacemos una predicción con el modelo\n",
    "        loss = criterion(outputs, labels)  # Calculamos la pérdida\n",
    "        loss.backward()                  # Propagamos los gradientes\n",
    "        optimizer.step()                 # Actualizamos los pesos del modelo\n",
    "\n",
    "    # Mostramos la pérdida al final de cada época\n",
    "    print(f\"Época [{epoch+1}/{epochs}], Pérdida: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluación del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  # Ponemos el modelo en modo evaluación (desactiva Dropout)\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():  # Desactivamos el cálculo de gradientes para evaluación\n",
    "    for images, labels in test_loader:  # Iteramos sobre los datos de prueba\n",
    "        outputs = model(images)         # Hacemos predicciones\n",
    "        _, predicted = torch.max(outputs.data, 1)  # Obtenemos la clase con mayor probabilidad\n",
    "        total += labels.size(0)         # Total de muestras evaluadas\n",
    "        correct += (predicted == labels).sum().item()  # Contamos las predicciones correctas\n",
    "\n",
    "# Calculamos y mostramos la precisión del modelo\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Accuracy en el conjunto de prueba: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datos pruebas ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1er intento -> sigmoid y adam: valores de pérdida mayores a 1.3 en las 5 épocas. Learning rate es 0.01, batch size 128. -> Accuracy 44.96% \n",
    "2do intento -> batch size 64, learning rate 0.001. Pérdida 0.29 es la mayor. -> Accuracy 96.62%\n",
    "3er intento -> batch size 128, learning rate 0.01, epoch 5. -> Accuracy 90.74% IMPORTANTE VER EL AUMENTO QUE TUVO. \n",
    "4to intento -> batch size 64, learning rate 0.01, epoch 5. Mayores valores de pérdida 1.1 el más bajo y alto 1.9. -> Accuracy 63.96%\n",
    "5to intento -> batch 128, learning rate 0.1, epoch 5. Perdida con valores de 2. -> Accuracy 11.35%\n",
    "\n",
    "sigmoid y SGD \n",
    "1er intento -> batch size 128, learning rate 0.01. Pérdida con valores grandes de 2. -> Accuracy 73.28%\n",
    "2do intento -> batch 64, learning rate 0.001. Perdida valores de 2. -> Accuracy 11.35%\n",
    "3r intento -> batch 128, learning 0.2. Perdida menores. -> Accuracy 96.98\n",
    "4to intento -> batch 128, learning 0.5. Perdida al principio mayores. de 2, pero luego 0.4. -> Accuracy 95.11%\n",
    "\n",
    "TANH Y SGD\n",
    "1er intento -> batch 128, learning 0.001. Perdida medias. -> Accuracy 91.90%\n",
    "2do intento -> 128, learning 0.01. Perdida menos.-> Accuracy 96.68%\n",
    "3er intento -> 128, learning 0.1, perdida 0.29. -> Accuracy 95%\n",
    "\n",
    "TANH Y ADAM\n",
    "1er intento -> batch 128, learning 0.001. Perdida 0.22.-> Accuracy 96.81%\n",
    "2do intento -> batch 64, learning 0.001. Pérdida 0.28.-> Accuracy 96.50%\n",
    "3er intento -> batch 64, learning 0.01. Pérdida 2.4.-> Accuracy 10.28%\n",
    "5to intento -> batch 64, learning 0.1. Pérdida 4.-> Accuracy 10.32%\n",
    "\n",
    "RELU Y ADAM\n",
    "1er intento -> batch 64, learning 0.001. Pérdida 0.3.-> Accuracy 97.13%\n",
    "2do intento -> batch 64, learning 0.01. Pérdida 0.6.-> Accuracy 87.60%\n",
    "3er intento -> batch 64, learning 0.1. Pérdida 2.3.-> Accuracy 11.35%\n",
    "4to intento aumenté epoch 10. -> batch 64, learning 0.005. Pérdida 0.42.-> Accuracy 95.50%\n",
    "5to intento epoch 10 -> batch 64, learning 0.001. Pérdida 0.23. -> Accuracy 97.61%\n",
    "\n",
    "RELU Y SGD \n",
    "1er intento -> batch 64, learning 0.001. Pérdida 0.7. -> Accuracy 96.18% \n",
    "2do intento -> batch 64, learning 0.01. Pérdida 0.28. -> Accuracy 97.97% EL MEJOR.\n",
    "3er intento -> batch 64, learning 0.1. Pérdida 2. -> Accuracy 19.95%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Informe ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego de los experimentos realizados, el que tuvo mejor accuracy fue la función de activación de Relu con optimizador SGD. EL resultado fue de 97.97%. Cuando se usó un learning rate muy alto el accuracy tendió a bajar mucho y en el caso de los datos que tenemos, de carácter MNIST y en que nuestro objetivo es clasificar y, que además no se requiere un learning rate alto ni complejizar el modelo. De hecho hacer ello hace que funcione de forma deficiente si lo medimos en base al accuracy que nos entrega. El learning rate ideal fue 0.01, como un punto medio entre un valor muy bajo 0.001 y muy alto 0.1. En este caso, se logró buena precisión del modelo para la clasificación y éste realizó los pasos adecuados y no saltos, como en el caso de 0.1 en que es muy rápido para capturar los detalles y por tanto, se los salta. Es así que un learning rate de 0.01 le otorga estabilidad al proceso y mejor clasificación. \n",
    "\n",
    "En el caso de probar la función de activación sigmoid con SGD, se puede observar que al juntar ambos el learning rate de mayor valor funciona mejor y, en este caso, el accuracy sería mayor. Por el contrario, al usar valores bajos como 0.001 se generan mayores pérdidas y baja accuracy. En los experimentos realizados con la función de activación de Tahn con SGD, pasa lo mismo que en el caso anterior en que al aumentar el learning rate el accuracy aumenta, pero el que cae mejor es el término medio de 0.01. \n",
    "\n",
    "Los mejores resultados se generaron con función de activación de Relu, ya que es más estable para lo que requiere este experimento de clasificar desde los datos MNIST. Se puede ver ello porque Relu funciona de buena forma tanto con SGD como con Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Redes convolucionales ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar si hay una GPU disponible, de lo contrario usar la CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Preprocesamiento: Definir transformaciones para los datos\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),                # Convertir imágenes a tensores\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalizar los valores a un rango de [-1, 1]\n",
    "])\n",
    "\n",
    "# Cargar el conjunto de datos MNIST\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)  # Datos de entrenamiento\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)  # Datos de prueba\n",
    "\n",
    "# Crear DataLoaders para manejar los datos de forma eficiente\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)   # Loader para entrenamiento (batch de 128, mezclado)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)    # Loader para prueba (batch de 128, sin mezclar)\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, verbose=False, filters_l1=32, filters_l2=64, dropout=0.2, final_layer_size=128):\n",
    "        super(CNN, self).__init__()\n",
    "        self.verbose = verbose\n",
    "        self.filters_l1 = filters_l1\n",
    "        self.filters_l2 = filters_l2\n",
    "        self.dropout_rate = dropout\n",
    "        self.final_layer_size = final_layer_size\n",
    "\n",
    "        # Primera capa convolucional\n",
    "        self.conv1 = nn.Conv2d(1, self.filters_l1, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Segunda capa convolucional\n",
    "        self.conv2 = nn.Conv2d(self.filters_l1, self.filters_l2, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # Calcular automáticamente las dimensiones de la capa lineal (fc1)\n",
    "        self.fc1_input_size = self._calculate_fc1_input_size()\n",
    "        \n",
    "        # Primera capa completamente conectada\n",
    "        self.fc1 = nn.Linear(self.fc1_input_size, self.final_layer_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_rate)\n",
    "        self.fc2 = nn.Linear(self.final_layer_size, 10)  # Capa de salida para 10 clases (MNIST)\n",
    "\n",
    "    def _calculate_fc1_input_size(self):\n",
    "        \"\"\"\n",
    "        Calcula automáticamente el tamaño de la entrada para la primera capa completamente conectada (fc1).\n",
    "        Simula una pasada con una imagen de prueba de tamaño (1, 28, 28).\n",
    "        \"\"\"\n",
    "        with torch.no_grad():  # Desactiva gradientes\n",
    "            x = torch.randn(1, 1, 28, 28)  # Tensor ficticio de entrada con tamaño MNIST (batch_size=1)\n",
    "            x = self.pool(torch.relu(self.conv1(x)))  # Aplicar Conv1 -> Pool\n",
    "            x = self.pool(torch.relu(self.conv2(x)))  # Aplicar Conv2 -> Pool\n",
    "            fc1_input_size = x.numel()  # Calcular número total de elementos\n",
    "        return fc1_input_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.verbose: \n",
    "            print(f\"Entrada: {x.shape}\")  # Imprime la dimensión de la entrada\n",
    "\n",
    "        # Primera capa convolucional, ReLU y MaxPooling\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        if self.verbose:\n",
    "            print(f\"Después de Conv1 y MaxPooling: {x.shape}\")  # Dimensión después de Conv1 y Pool\n",
    "\n",
    "        # Segunda capa convolucional, ReLU y MaxPooling\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        if self.verbose:\n",
    "            print(f\"Después de Conv2 y MaxPooling: {x.shape}\")  # Dimensión después de Conv2 y Pool\n",
    "\n",
    "        # Aplanar las características 2D a 1D\n",
    "        x = x.view(-1, self.fc1_input_size)\n",
    "        if self.verbose:\n",
    "            print(f\"Después de Aplanamiento: {x.shape}\")  # Dimensión después de Flatten\n",
    "\n",
    "        # Primera capa completamente conectada\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        if self.verbose:\n",
    "            print(f\"Después de Fully Connected (fc1): {x.shape}\")  # Dimensión después de fc1\n",
    "\n",
    "        # Aplicar Dropout\n",
    "        x = self.dropout(x)\n",
    "        if self.verbose:\n",
    "            print(f\"Después de Dropout: {x.shape}\")  # Dimensión después de Dropout\n",
    "\n",
    "        # Capa de salida\n",
    "        x = self.fc2(x)\n",
    "        if self.verbose:\n",
    "            print(f\"Después de Fully Connected (fc2): {x.shape}\")  # Dimensión después de fc2 (salida final)\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar el modelo, la función de pérdida y el optimizador\n",
    "model = CNN(verbose=False, filters_l1=8, filters_l2=32, dropout=0.2, final_layer_size=128).to(device)       # Mover el modelo a la GPU/CPU\n",
    "criterion = nn.CrossEntropyLoss()                    # Función de pérdida para clasificación multiclase\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001) # Optimizador Adam con tasa de aprendizaje 0.001\n",
    "\n",
    "# Definir la función de entrenamiento\n",
    "def train(model, loader, criterion, optimizer, device):\n",
    "    model.train()  # Establecer el modelo en modo de entrenamiento\n",
    "    running_loss = 0.0\n",
    "    for images, labels in loader:  # Iterar sobre los lotes de datos\n",
    "        images, labels = images.to(device), labels.to(device)  # Mover los datos a la GPU/CPU\n",
    "\n",
    "        optimizer.zero_grad()       # Reiniciar los gradientes\n",
    "        outputs = model(images)     # Paso hacia adelante\n",
    "        loss = criterion(outputs, labels)  # Calcular la pérdida\n",
    "        loss.backward()             # Paso hacia atrás (cálculo de gradientes)\n",
    "        optimizer.step()            # Actualizar los pesos\n",
    "\n",
    "        running_loss += loss.item()  # Acumular la pérdida\n",
    "    return running_loss / len(loader)  # Devolver la pérdida promedio\n",
    "\n",
    "# Definir la función de evaluación\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()  # Establecer el modelo en modo de evaluación\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # Deshabilitar el cálculo de gradientes para ahorrar memoria\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)  # Mover datos a la GPU/CPU\n",
    "            outputs = model(images)  # Paso hacia adelante\n",
    "            _, predicted = torch.max(outputs, 1)  # Obtener las predicciones (clase con mayor probabilidad)\n",
    "            total += labels.size(0)  # Contar el número total de ejemplos\n",
    "            correct += (predicted == labels).sum().item()  # Contar las predicciones correctas\n",
    "    return correct / total  # Calcular la precisión\n",
    "\n",
    "# Bucle principal de entrenamiento\n",
    "num_epochs = 10  # Número de épocas\n",
    "for epoch in range(num_epochs):\n",
    "    # Entrenar el modelo y calcular la pérdida\n",
    "    train_loss = train(model, train_loader, criterion, optimizer, device)\n",
    "    # Evaluar el modelo en el conjunto de prueba\n",
    "    test_accuracy = evaluate(model, test_loader, device)\n",
    "    # Imprimir los resultados de la época actual\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Calcular la precisión final en el conjunto de prueba\n",
    "final_accuracy = evaluate(model, test_loader, device)\n",
    "print(f\"Final Test Accuracy: {final_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datos pruebas ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RELU Y ADAM\n",
    "En la arquitectura original  Accuracy: 0.9910. Filtros: filters_l1=8, filters_l2=32. Dropout 0.2, layer size 128. Learning rate 0.001\n",
    "pruebas: \n",
    "RELU Y ADAM. l1 = 32, l2=64, l3=128, l4= 256, Dropout 0.4, layer size 256. LR 0.01. -> Accuracy 0.1135. Ahora son 4 capas convolucionales.\n",
    "RELU Y ADAM. l1 = 32, l2=64, l3=128, l4= 256, Dropout 0.4, layer size 256. LR 0.1. -> Accuracy 0.009\n",
    "l1 = 32, l2=64, l3=128, l4= 256, Dropout 0.3, layer size 256. LR 0.005. -> Accuracy 0.9862\n",
    "\n",
    "Sigmoid y SGD -> l1 = 32, l2=64, l3=128, l4= 256, Dropout 0.4, layer size 256. LR 0.001.-> Accuracy 0.10\n",
    "l1 = 64, l2=128, l3=256, l4= 512, Dropout 0.2, layer size 512. LR 0.001. -> Accuracy 0.1135\n",
    "l1 = 64, l2=128, l3=256, l4= 512, Dropout 0.5, layer size 512. LR 0.1. -> Accuracy 0.0958\n",
    "\n",
    "softplus y sgd -> l1 = 64, l2=128, l3=256, l4= 512, Dropout 0.2, layer size 512. LR 0.1. -> Accuracy 0.1135\n",
    "l1 = 64, l2=128, l3=256, l4= 512, Dropout 0.2, layer size 512. LR 0.001. -> Accuracy 0.0980\n",
    "l1=32, l2=64, l3=128, l4= 256, dropout=0.2, final_layer_size=256, LR 0.001. -> Accuracy 0.1135\n",
    "\n",
    "Disminuir numeros capas. \n",
    "softplus y Adam -> l1=32, l2=64, l3=128, l4= 256, dropout=0.2, final_layer_size=256, LR 0.001. -> Accuracy 0.9875\n",
    "l1=32, l2=64, l3=128, l4= 256, dropout=0.2, final_layer_size=256, LR 0.01.-> Accuracy 0.1135\n",
    "\n",
    "softplus y RMSprOP -> l1=32, l2=64, l3=128, l4= 256, dropout=0.2, final_layer_size=256, LR 0.001.-> Accuracy 0.9824\n",
    "l1 = 64, l2=128, l3=256, l4= 512, Dropout 0.2, layer size 512, LR 0.001.-> Accuracy 0.9743\n",
    "\n",
    "Tahn y adam -> l1=32, l2=64, l3=128, l4= 256, dropout=0.2, final_layer_size=256, LR 0.001.-> Accuracy 0.9903\n",
    "l1=32, l2=64, l3=128, l4= 256, dropout=0.1, final_layer_size=256, LR 0.01.-> Accuracy 0.1032\n",
    "l1=8, l2=32, l3=64, l4= 128, dropout=0.2, final_layer_size=128, LR 0.005.-> Accuracy 0.9742\n",
    "l1=8, l2=32, l3=64, l4= 128, dropout=0.2, final_layer_size=128, LR 0.001.-> Accuracy 0.9868\n",
    "\n",
    "Tahn y SDG -> l1=32, l2=64, l3=128, l4= 256, dropout=0.2, final_layer_size=256, LR 0.001.-> Accuracy 0.9527\n",
    "l1=32, l2=64, l3=128, l4= 256, dropout=0.3, final_layer_size=256, LR 0.005.-> Accuracy 0.9866\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Informe ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La arquitectura original tenía 2 capas y además sus valores eran bajos, éstos fueron aumentado a término medio partiendo desde 32 y el rendimiento mejoró. Aún así con todos los experimentos realizados no se logró encontrar una arquitectura mejor en que el accuracy aumentara. Lo que sí se pudo observar es que al aumentar el valor de las capas y partir de valores altos como 64, esto generó que en la mayoria de los casos el accuracy fue muy bajo y por tanto, de rendimiento bajo. Por lo mismo, al principio no resultó mezclar las distintas funciones de activación con las funciones de optimización, pero el error fue  que los valores de las capas eran muy grandes y no compatibilizó con las características que tiene el modelo. En que no requiere de redes complejas ni varias capas, de hecho funciona de forma óptima con parámetros simples. Asimismo, luego de bajar los valores, todos los experimentos dieron un buen porcentaje de accuracy pero en el caso de aumentar el learning rate y que el optimizador fuera Adam, los valores de accuracy bajaron mucho. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparación ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tal como se mencionó anteriormente, el objetivo que quiere alcanzar este modelo con datos de tipo MNIST se logra no con un modelo que tenga mucha complejidad, de hecho al agregarle complejidad, aumentar el número de capas y el valor de los filtros el modelo no entregó resultados óptimos y por tanto, no rindió como queríamos. Respecto a los tiempos, pude notar que el tiempo aumentó si los filtros de las capas tenían un valor grande. Normalmente, tardó en 5 epochs 6 minutos y medio. En el caso de los experimentos con learning rate de 0.01 disminuyó a 4 minutos y medio. Con mayores capas el tiempo fue de 13 minutos y medio. Y, con la menor cantidad de capas y valor de los filtros el tiempo fue menor de 2 minutos y medio. En su mayoría se experimentó con 5 epochs para así lograr evaluar diferentes arquitecturas y parámetros de forma más rápida. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
